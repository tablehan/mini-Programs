{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b9c6d0-1b00-4f5d-9de1-bf8f45bd8197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fbd1aa-46f2-44be-8dcb-6cc5056bdcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89baa01-8e52-486a-93c4-c36b942a9459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/19 09:58:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# yarn mode\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config('spark.executor.instances','999')\\\n",
    "        .config('spark.executor.memory','5500M')\\\n",
    "        .appName(\"thchow\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f287e4a3-dc70-450a-b7e9-d34d012fbfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thchow'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check spark app name\n",
    "spark.sparkContext.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae626a8b-7c45-42bb-ad15-bcd49771c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de17789-319a-4190-b315-31ef7fe4f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.set_option(\"compute.default_index_type\", \"distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d67988-2ff7-4902-b33e-e8e8063c1191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.6 (main, Aug 10 2022, 11:40:04) [GCC 11.3.0]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print runtime versions\n",
    "# Python version\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0565775-fb61-4fbf-b7ea-e7250c96245d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark version\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a6ee658-7fa1-4277-8bdd-23e2d88bf41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "# load iris.csv into Spark dataframe\n",
    "#df = spark.read.csv('file:///vagrant/data/iris.csv', header=True, inferSchema=True)\n",
    "df = ps.read_csv('/tmp/grouped_simplified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0694ff0-a6f9-48fa-bdf8-ebd85af98767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "sdf=df.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "907929d1-d263-419c-8bfb-63e2014498b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+-------+-------+-------+--------+\n",
      "|  open_day|crypto|   open|   high|    low|  close|  volume|\n",
      "+----------+------+-------+-------+-------+-------+--------+\n",
      "|2020-04-07|  DATA|0.04088|0.08999|0.04088|0.04741| 21344.9|\n",
      "|2020-04-07|  DATA|0.04741|0.04741|0.04539|0.04539| 18031.6|\n",
      "|2020-04-07|  DATA|0.04541|   0.06|0.04541|0.05154|360683.7|\n",
      "|2020-04-07|  DATA|0.04698|0.05047|0.04636|0.04651|209030.1|\n",
      "|2020-04-07|  DATA|0.04671|0.04671| 0.0461| 0.0461| 95900.6|\n",
      "|2020-04-07|  DATA|0.04648|0.04701|0.04625|  0.047|105975.6|\n",
      "|2020-04-07|  DATA| 0.0467|0.04701|0.04657|0.04678| 89238.7|\n",
      "|2020-04-07|  DATA|0.04694|0.04737|0.04662|0.04737|113921.5|\n",
      "|2020-04-07|  DATA|0.04737|0.04775|0.04737|0.04762| 33868.8|\n",
      "|2020-04-07|  DATA|0.04782|0.04875|  0.047|  0.047|249964.1|\n",
      "|2020-04-07|  DATA|0.04753|0.04753|0.04701|0.04701| 29641.0|\n",
      "|2020-04-07|  DATA|0.04706| 0.0475|  0.047|  0.047|201356.0|\n",
      "|2020-04-07|  DATA|0.04699|  0.047|0.04662|0.04662| 69740.6|\n",
      "|2020-04-07|  DATA|0.04665|0.04707|0.04665|0.04707|  3461.6|\n",
      "|2020-04-07|  DATA|0.04675|0.04701|0.04663|0.04663| 69401.9|\n",
      "|2020-04-07|  DATA|0.04661|0.04661|0.04577|0.04577| 25038.8|\n",
      "|2020-04-07|  DATA|0.04577|0.04577|0.04542|0.04561| 49117.0|\n",
      "|2020-04-07|  DATA|0.04571|0.04614|0.04571|0.04614| 29181.0|\n",
      "|2020-04-07|  DATA|0.04635|0.04655|0.04635|0.04655| 63277.6|\n",
      "|2020-04-07|  DATA|0.04655|0.04672|0.04655|0.04671| 56384.4|\n",
      "+----------+------+-------+-------+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.createOrReplaceTempView(\"table1\")\n",
    "sdf2 = spark.sql(\"SELECT from_unixtime(open_time/1000, 'yyyy-MM-dd') as open_day, crypto, open, high, low, close, volume from table1\")\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f069732c-a58a-42a0-a201-9309129c0e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(162 + 1) / 163]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+------+------+------------+\n",
      "|  open_day|crypto|  open|  high|   low| close|      volume|\n",
      "+----------+------+------+------+------+------+------------+\n",
      "|2020-05-23|  DATA|0.0712|0.0723|0.0704|0.0717|   3299715.8|\n",
      "|2020-05-26|  DATA|0.0726|0.0729|0.0699|0.0701|   3579597.6|\n",
      "|2020-06-13|  DATA|0.0567| 0.065|0.0561|0.0583|1.08368861E7|\n",
      "|2020-09-02|  DATA| 0.059| 0.059|0.0532|0.0536|   3204210.0|\n",
      "|2020-12-15|  DATA|0.0408|0.0424|0.0404|0.0407|   8683413.9|\n",
      "|2021-12-26|  DATA|0.1236|0.1239|0.1195|0.1213|1.02392535E7|\n",
      "|2022-02-07|  DATA| 0.092|0.0947|0.0842|0.0927|5.59320658E7|\n",
      "|2022-02-13|  DATA|0.0828|0.0878|0.0826|0.0858|   9138100.6|\n",
      "|2022-04-08|  DATA|0.0856|0.0876|0.0805|0.0837|7.89970345E7|\n",
      "|2020-12-02|  COMP|108.95|110.86|104.11|110.18|  28394.8845|\n",
      "|2021-01-25|  DATA|0.0624| 0.063|0.0583|0.0593|1.75427117E7|\n",
      "|2021-01-27|  DATA|0.0555|0.0615|0.0508| 0.054| 1.5222135E7|\n",
      "|2022-05-05|  DATA|0.0632|0.0672|0.0605|0.0607|1.94884531E7|\n",
      "|2022-07-15|  DATA|0.0311|0.0322|0.0303|0.0313| 4.1822803E7|\n",
      "|2022-08-28|  DATA|0.0307|0.0316|  0.03|0.0305|1.48041621E7|\n",
      "|2021-03-23|  COMP|417.12|419.73| 372.4|386.01|  45429.2444|\n",
      "|2021-12-20|  COMP| 203.7| 205.9| 181.6| 184.4|   55023.265|\n",
      "|2020-09-08|  DATA|0.0408|0.0425|0.0393|0.0404|   1264368.6|\n",
      "|2022-03-12|  DATA|0.0728| 0.073|0.0692|  0.07|1.75560773E7|\n",
      "|2022-07-31|  DATA| 0.039|0.0409|0.0377|0.0397|3.13373296E7|\n",
      "+----------+------+------+------+------+------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf2.createOrReplaceTempView(\"table2\")\n",
    "sdf3 = spark.sql(\"SELECT open_day, crypto, round(first(open),4) as open, round(max(high),4) as high, round(min(low),4) as low, round(last(close),4) as close, round(sum(volume),4) as volume from table2 group by crypto, open_day\")\n",
    "sdf3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa50da1-f214-4502-a652-d3140d994920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
